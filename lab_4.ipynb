{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b38396-4691-413b-9856-edd71a08c288",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab4 Intro\n",
    "Now that you have classified the Voice of Customers into categories in the previous experiment, you can use Generative AI techniques to write analysis report for you further.\n",
    "\n",
    "### Your objectives are:\n",
    "\n",
    "- Explore the statistical results of classification\n",
    "- Write a summary analysis report\n",
    "- Tuning the prompt instruction to enhance the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885f205-3b90-4d03-8c9d-0e763047c192",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "- If you experince \"ERROR: pip's dependency resolver does not currently...\", please just ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f625de-1ca3-4167-a158-2628994ff090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq boto3 langchain==0.2.16 langchain_aws==0.1.17 pandas==2.2.2 openpyxl==3.1.5 termcolor==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a39f8-d017-4bd3-b289-dab163db7da2",
   "metadata": {},
   "source": [
    "## 2. Initialize Bedrock model using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80647c-214f-4b5b-bea1-5676af790fc3",
   "metadata": {},
   "source": [
    "We will continue to use Amazon Foundation Model to help generate analysis for the result. \n",
    "- We use [Langchain](https://www.langchain.com/) SDK to build the application\n",
    "- Initialize a ChatBedrock object with Amzon Foundation model, the model id is <span style=\"color: blue;\">\"amazon.nova-pro-v1:0\"</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f0143-4542-4502-a087-ec81c4834ad1",
   "metadata": {},
   "source": [
    "#### Customzie a LangChain ChatModel Class\n",
    "- As the time of this event, LangChain has not supported the latest Amazon Foundation Model yet, we will customize a LangChain Chat Model Class, so that the latest model can be integrated with the chain prompting in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386c9c2-e7e3-4d9b-ba82-39a1905310f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage,AIMessage,SystemMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser,XMLOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder,HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "class ChatModelNova(BaseChatModel):\n",
    "\n",
    "    model_name: str\n",
    "    br_runtime : Any = None\n",
    "    ak: str = None\n",
    "    sk: str = None\n",
    "    region:str = None\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "\n",
    "        if not self.br_runtime:\n",
    "            if self.ak and self.sk:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime',\n",
    "                                               region_name = self.region,\n",
    "                                              aws_access_key_id = self.ak,\n",
    "                                               aws_secret_access_key = self.sk\n",
    "            \n",
    "                                              )\n",
    "\n",
    "            else:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime')\n",
    "            \n",
    "        \n",
    "        new_messages = []\n",
    "        system_message = ''\n",
    "        for msg in messages:\n",
    "            if isinstance(msg,SystemMessage):\n",
    "                system_message = msg.content\n",
    "            elif isinstance(msg,HumanMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "            elif isinstance(msg,AIMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "\n",
    "        \n",
    "        temperature = kwargs.get('temperature',0.5)\n",
    "        maxTokens = kwargs.get('max_tokens',3000)\n",
    "\n",
    "        #Base inference parameters to use.\n",
    "        inference_config = {\"temperature\": temperature,\"maxTokens\":maxTokens}\n",
    "\n",
    "\n",
    "        # Send the message.\n",
    "        response = self.br_runtime.converse(\n",
    "            modelId=self.model_name,\n",
    "            messages=new_messages,\n",
    "            system=[{\"text\" : system_message}] if system_message else [],\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        output_message = response['output']['message']\n",
    "\n",
    "        message = AIMessage(\n",
    "            content=output_message['content'][0]['text'],\n",
    "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
    "            response_metadata={  # Use for response metadata\n",
    "                **response['usage']\n",
    "            },\n",
    "        )\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        if not self.br_runtime:\n",
    "            if self.ak and self.sk:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime',\n",
    "                                               region_name = self.region,\n",
    "                                              aws_access_key_id = self.ak,\n",
    "                                               aws_secret_access_key = self.sk\n",
    "                                                          )\n",
    "\n",
    "            else:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime')\n",
    "            \n",
    "        \n",
    "        new_messages = []\n",
    "        system_message = ''\n",
    "        for msg in messages:\n",
    "            if isinstance(msg,SystemMessage):\n",
    "                system_message = msg.content\n",
    "            elif isinstance(msg,HumanMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "            elif isinstance(msg,AIMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "\n",
    "        \n",
    "        temperature = kwargs.get('temperature',0.5)\n",
    "        maxTokens = kwargs.get('max_tokens',3000)\n",
    "\n",
    "        #Base inference parameters to use.\n",
    "        inference_config = {\"temperature\": temperature,\"maxTokens\":maxTokens}\n",
    "\n",
    "        # Send the message.\n",
    "        streaming_response = self.br_runtime.converse_stream(\n",
    "            modelId=self.model_name,\n",
    "            messages=new_messages,\n",
    "            system=[{\"text\" : system_message}] if system_message else [],\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        # Extract and print the streamed response text in real-time.\n",
    "        for event in streaming_response[\"stream\"]:\n",
    "            if \"contentBlockDelta\" in event:\n",
    "                text = event[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "                # print(text, end=\"\")\n",
    "                chunk = ChatGenerationChunk(message=AIMessageChunk(content=[{\"type\":\"text\",\"text\":text}]))\n",
    "\n",
    "                if run_manager:\n",
    "                    # This is optional in newer versions of LangChain\n",
    "                    # The on_llm_new_token will be called automatically\n",
    "                    run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "                yield chunk\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                # Let's add some other information (e.g., response metadata)\n",
    "                chunk = ChatGenerationChunk(\n",
    "                    message=AIMessageChunk(content=[], response_metadata={**metadata})\n",
    "                )\n",
    "                if run_manager:\n",
    "\n",
    "                    run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "                yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "\n",
    "llm = ChatModelNova(model_name=\"amazon.nova-pro-v1:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f18fa1-aa33-4585-956f-697225850adc",
   "metadata": {},
   "source": [
    "- test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e72624-2669-441e-92b4-ade4d14229a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"human\", \"translate to french: I love programming.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76b2c8-dda9-4a86-b832-d6f236781023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk in llm.stream(messages):\n",
    "    if chunk.content and chunk.content[0].get('type') == 'text':\n",
    "        print(chunk.content[0]['text'],end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de2c96-c8ec-486a-ac7d-07e792132184",
   "metadata": {},
   "source": [
    "## 3. Load the VOC classification results data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743aa7ea",
   "metadata": {},
   "source": [
    "At the end of Lab 3, we saved the classified results data into result_lab_3.csv file. To begin our analysis, we first need to load this results data and review its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce8465-7a4e-4bbf-8f0f-bb685b1cc7dd",
   "metadata": {},
   "source": [
    "- Load the classification result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6c2db-4e68-42e9-9c03-0eba22946678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab0d35-c5ef-4860-a81d-cc24abff088c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('result_lab_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb271e-c1bc-4c9a-94ac-53e7cbc6aba8",
   "metadata": {},
   "source": [
    "- Use columns 'id','summary','reason','predict_label' \n",
    "\n",
    "For analysis purpose, we only need data from column 'summary', 'reason', and 'predict_label'. Execute the code below to retrieve these columns data and rename the column 'predict_label' to 'category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06de88-26b3-44a0-9823-b03abf7b1752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = raw_data[['summary','reason','predict_label']].rename(columns={'predict_label':'category'})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb963e0-765c-41a6-b8aa-99db5a135dbe",
   "metadata": {},
   "source": [
    "## 4. A simple summary report\n",
    "\n",
    "We have defined a simple system prompt and user prompt to instruct Amazon Foundation model to generate a analysis report. The results data will be incorporated into the prompt as a variable, allowing the model to access and utilize this information in its report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38162527-87f5-4522-b9f0-19382c1fd4f7",
   "metadata": {},
   "source": [
    "### 4.1 Define system prompt and user prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cce5e8-af27-4519-88a5-169b823467ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = \"\"\"\n",
    "You are a data analysis expert in the retail industry. \n",
    "Your task is to analyze the given data and write a narrative summary analysis report.\n",
    "Follow these instructions:\n",
    "\n",
    "<instructions>\n",
    "- Transform the data into natural language, including all key data points as much as possible\n",
    "- Only provide the final narrative report, do not show any intermediate analysis steps or processes\n",
    "- Give insights and interpretations about the data in your narrative\n",
    "</instructions>\n",
    "\n",
    "Here is the tabular data in json to analyze:\n",
    "<data>\n",
    "{tabular}\n",
    "</data>\n",
    "\n",
    "Please summarize the data in a narrative report format, following the instructions above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489deef-654c-47b1-9d4a-29710e1ef95f",
   "metadata": {},
   "source": [
    "- Create a langchain chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2dc4b-bede-4e59-af62-af9ca22ada36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_simple = ChatPromptTemplate([\n",
    "    ('user',user),\n",
    "    ])\n",
    "chain = prompt_simple | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9e389-b95a-4f2a-a150-70d6587b2b06",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Generate the report\n",
    "- convert the tabular data to json string and pass them to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784210a-a583-4c83-876a-8d953abad4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = ''\n",
    "for chunk in chain.stream({\"tabular\":results.to_json()}):\n",
    "    print(colored(chunk,\"green\"),end='',flush=True)\n",
    "    response += chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3438b-6347-4d48-a0f2-35ab1e88b732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42481282-4066-433c-acce-fb42bcb8be71",
   "metadata": {},
   "source": [
    "## 5. Advanced version of report\n",
    "\n",
    "The example provided above is a basic demonstration of the analysis generation capabilities of a LLM (Large Language Model). Now let's craft the prompt to generate a more comprehensize, concise, and detailed version of report.\n",
    "\n",
    "The prompt provided below are much more detailed and contains steps-by-steps instruction and clear structure that can guide the LLM to generate a report rich in information. Additionally, we have supplied the LLM with statistical data, specifically the summary numbers of comments by category, to provide further context and enhance the depth of the analysis in the report.\n",
    "\n",
    "- Add formation instruction and statistic data to enhance the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62662f4-6ed3-4154-a2af-a60066d570eb",
   "metadata": {},
   "source": [
    "- Let's stat the number of comments by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0bf18-2f66-428f-b0c3-a294b087820d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='category', data=results[['category']])\n",
    "plt.title('Frequency of Category Labels')\n",
    "plt.xlabel('Category Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "statistic_label = results.pivot_table(index='category',aggfunc='count')\n",
    "statistic_label[['summary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6ea82-10f2-4790-af4d-d123be6904bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Define system prompt and user prompt template\n",
    "- we add instruction to generate report in certain content agenda as:\n",
    "```markdown\n",
    "# Executive Summary\n",
    "Provide a brief overview of the report, summarizing key findings, conclusions, and recommendations. This section should be concise, allowing readers to grasp the main points quickly.\n",
    "\n",
    "# Data Presentation\n",
    "Data Overview: Provide a summary of the datasets used, including key metrics and attributes.\n",
    "Key Performance Indicators (KPIs): Highlight the KPIs that are critical to the analysis.\n",
    "\n",
    "# Findings\n",
    "Present the results of the analysis in a structured manner:\n",
    "Descriptive Statistics: Summarize the main characteristics of the data.\n",
    "Trends and Patterns: Discuss any significant trends or patterns identified.\n",
    "Comparative Analysis: If applicable, compare different datasets or time periods.\n",
    "\n",
    "# Discussion\n",
    "Interpret the findings in the context of the initial questions posed in the introduction.\n",
    "Discuss the implications of the results, including potential impacts on business strategies or operations.\n",
    "Address any unexpected results or anomalies.\n",
    "\n",
    "# Conclusions\n",
    "Summarize the key insights derived from the analysis.\n",
    "Restate the significance of the findings in relation to the report’s objectives.\n",
    "\n",
    "# Recommendations\n",
    "Provide actionable recommendations based on the findings. These should be specific and feasible, guiding stakeholders on the next steps.\n",
    "```\n",
    "\n",
    "- And add additional context of statistic data in json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63431d3-ba2a-493a-aed5-a01e5e46636e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = \"\"\"\n",
    "You are a data analysis expert in the retail industry\n",
    "Your task is to analyze the given data and write a data analysis report. Follow these instructions:\n",
    "\n",
    "- instructions:\n",
    "- Transform the data into natural language, including all key data points as much as possible\n",
    "- Only provide the final narrative report, do not show any intermediate analysis steps or processes\n",
    "- Give insights and interpretations about the data in your narrative|\n",
    "- the report should follow the content agenda as below:\n",
    "\n",
    "# Executive Summary\n",
    "Provide a brief overview of the report, summarizing key findings, conclusions, and recommendations. \n",
    "This section should be concise, no more than 100 words.\n",
    "\n",
    "# Data Presentation\n",
    "Data Overview: Provide a summary of the datasets used, including key metrics and attributes.\n",
    "Key Performance Indicators (KPIs): Highlight the KPIs that are critical to the analysis.\n",
    "\n",
    "# Findings\n",
    "Present the results of the analysis in a structured manner:\n",
    "Descriptive Statistics: Summarize the main characteristics of the data.\n",
    "Trends and Patterns: Discuss any significant trends or patterns identified.\n",
    "Comparative Analysis: If applicable, compare different datasets or time periods.\n",
    "\n",
    "# Discussion\n",
    "Interpret the findings in the context of the initial questions posed in the introduction.\n",
    "Discuss the implications of the results, including potential impacts on business strategies or operations.\n",
    "Address any unexpected results or anomalies.\n",
    "\n",
    "# Conclusions\n",
    "Summarize the key insights derived from the analysis.\n",
    "Restate the significance of the findings in relation to the report’s objectives.\n",
    "\n",
    "# Recommendations\n",
    "Provide actionable recommendations based on the findings. These should be specific and feasible, guiding stakeholders on the next steps.\n",
    "\n",
    "\n",
    "Here is the tabular data in json to analyze:\n",
    "{tabular}\n",
    "\n",
    "Here are the statistic data in json:\n",
    "{statistic}\n",
    "\n",
    "Please generate the report in markdown format:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd087dc-4da6-4418-8109-9c395abec3d3",
   "metadata": {},
   "source": [
    "- Create a langchain chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53018dbe-a33e-4d98-936c-760f520b2631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_advanced_1 = ChatPromptTemplate([\n",
    "    ('user',user),\n",
    "    ])\n",
    "chain_advanced = prompt_advanced_1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0781a-05c5-4f14-b330-7a1633996fc2",
   "metadata": {},
   "source": [
    "- convert the tabular data and statistic to json string and pass them to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeca0a7-df9f-46aa-a0f5-109505e1166e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_advanced = ''\n",
    "for chunk in chain_advanced.stream({\"tabular\":results.to_json(),\n",
    "                          'statistic':statistic_label['summary'].to_json()}):\n",
    "    print(colored(chunk,\"green\"),end='',flush=True)\n",
    "    response_advanced += chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34a6cb-680a-43d0-bff5-7da59bba2656",
   "metadata": {},
   "source": [
    "### 5.2 Display the report in markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d26e9-925b-46c1-b9e2-fad4fd40d468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display,HTML\n",
    "display(Markdown(response_advanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f75b45-9cdc-45c6-822d-b003c1b7d127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
