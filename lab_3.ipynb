{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2c4067-1006-4286-8965-84ebdcf119d3",
   "metadata": {},
   "source": [
    "# Lab3 Intro\n",
    "### Using RAG technique to enhance the solution by retrieving and injecting few shot examples into the prompt\n",
    "### What is RAG?\n",
    "RAG is a technique for augmenting LLM (Large Language Model) knowledge with additional data.\n",
    "Large Language Models (LLMs) can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "- **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "- **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "### What is Few-Shot Learning?\n",
    "Few-shot learning is a machine learning paradigm that enables models to generalize from a limited number of labeled examples. Unlike traditional supervised learning, which requires large datasets for training, few-shot learning allows models to adapt to new tasks or classes with minimal data. This is akin to how humans can learn new concepts quickly from just a few instances.\n",
    "\n",
    "### Using RAG and Few-Shot Learning to Classify VoC\n",
    "With the introduction of RAG and Few-Shot Learning, we can now apply these concepts practically. In this lab, we will utilize the RAG technique together with Few-Shot Learning to classify Voice of Customer data. This session will build upon the skills developed in the last two labs, as we will continue to employ prompt engineering and embedding techniques. \n",
    "\n",
    "### Your objectives are:\n",
    "- Setup a vector database using [Chroma integration in LangChain](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/) \n",
    "- Use Amazon Titan embedding model to embed the labeled examples and store into the vector database\n",
    "- Build a RAG chain that retrieves top k most relevant examples from vector store and generate the classification result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e150a-5129-4c24-9705-514bde6155ed",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "- If you experince \"ERROR: pip's dependency resolver does not currently...\", please just ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cc713-90ae-4137-b16a-b60e4a085fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.2.16 langchain_aws==0.1.17 pandas==2.2.2 openpyxl==3.1.5 chromadb==0.5.5 langchain-chroma==0.1.2 python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13504ce9-725d-49b4-84f7-24021777b9be",
   "metadata": {},
   "source": [
    "## 2. Initialize Bedrock model using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340ca2a-d688-4b0e-a8ee-4f976f426ba3",
   "metadata": {},
   "source": [
    "We will utilize the same approach used in Lab2 to setup the embedding object. \n",
    "- We use [Langchain](https://www.langchain.com/) SDK to build the application\n",
    "- Initialize a BedrockEmbeddings object with 'Titan Text Embeddings V2\" with the model id \"amazon.titan-embed-text-v2:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf99ee-3327-4329-8012-545f9b01955b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "bedrock_embedding = BedrockEmbeddings(model_id='amazon.titan-embed-text-v2:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2a870-e722-450b-be68-326c7ab4f4dd",
   "metadata": {},
   "source": [
    "- test run and preview the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12961966-0c38-423b-b3ed-5891d1804027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embedding = bedrock_embedding.embed_documents(['I love programing'])\n",
    "print(f\"The embedding dimension is {len(test_embedding[0])}, first 10 elements are: {test_embedding[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af28d3a-7852-499d-91d6-2dab52998228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6043d49a-537f-4ea1-b988-335898e496b8",
   "metadata": {},
   "source": [
    "## 3. Preparing the Dataset to be classified. \n",
    "- The data (Voice of Customers) is subject to experiment usage only\n",
    "\n",
    "We will once again load the categories.csv and comments.csv data files. In addition, we will introduce a new dataset, examples_with_label.csv, in this session. This new dataset provides an augmented knowledge base for the LLM, equipping it with relevant references. By incorporating these labeled examples into our workflow, it is expected to enhance the model's understanding and improve the accuracy of its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee159e-5ae3-4939-9666-a38afca37e58",
   "metadata": {},
   "source": [
    "- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6afaa-2b6c-4b73-8355-fa56e32a3ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2d500-df6e-4487-9d1a-d3cddd36d9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_definition = \"data/categories.csv\"\n",
    "categories = pd.read_csv(category_definition)\n",
    "display(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ba80e-b4f9-4da0-a502-a60414478af1",
   "metadata": {},
   "source": [
    "### Load the customer review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7c088-9670-40c8-aeed-0561e2e7267c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments_filepath = \"data/comments.csv\"\n",
    "comments = pd.read_csv(comments_filepath)\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec133291-df1b-4da1-a7c3-dd596c981d57",
   "metadata": {},
   "source": [
    "### Load examples data as we will use them as few shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771a792-fe05-4d10-a266-d42d0e621949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples_filepath = \"data/examples_with_label.csv\"\n",
    "examples_df = pd.read_csv(examples_filepath) \n",
    "examples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add973da-1fdc-4a4a-83f6-d998f081ab00",
   "metadata": {},
   "source": [
    "## 4. Create Vector database to store the embedding vectors of customer review text\n",
    "\n",
    "### 4.1 We will setup a vector database using [Chroma integration in LangChain](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/) \n",
    "- Chroma is the AI-native open-source vector database. This is lightweight vector database suitable for developers to quickly develop and experiment with applications.\n",
    "- We will use Chroma for our workshop experiment this time, but when you will develop your own application, there are variety of selections, such as Amazon OpenSearch, Amazon Aurora, Pinecone, Milvus,Faiss, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18be5f-6cac-49ed-9b2e-c39335c67ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "# You can uncomment below code to reset the vector db, if you want to retry more times\n",
    "# vector_store.reset_collection()  \n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=bedrock_embedding,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0a41c-d3ef-41d9-8f6f-036506a1accc",
   "metadata": {},
   "source": [
    "#### Let's consider each customer comment as an individual document, and then use the Langchain-Chroma SDK to upload all these customer comments into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93498c76-f5a2-437c-a095-2fafa54dbddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import hashlib\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de990cf6-e240-4fa9-beb7-eab472ed7802",
   "metadata": {},
   "source": [
    "#### build langchain documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1f3cd-fcc2-42cc-ab9f-ca5064ec2f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for comment,groundtruth in examples_df.values:\n",
    "    documents.append(\n",
    "        Document(\n",
    "        page_content=comment,\n",
    "        metadata={\"groundtruth\":groundtruth}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6126710-89d9-4059-8043-432d3fc38533",
   "metadata": {},
   "source": [
    "#### add documents to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059f362-3a9a-4a4f-a339-d750d75a7316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hash_ids = [hashlib.md5(doc.page_content.encode()).hexdigest() for doc in documents]\n",
    "vector_store.add_documents(documents=documents, ids=hash_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17e440-e71b-4b54-858f-94fe884cf7aa",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 We will use vector search to compare the similarity between them and choose the category that is the similarest to the customer review text\n",
    "\n",
    "Once the embedded customer review texts are stored in the vector database, we can apply the techniques we explored in Lab 2, semantic similarity search, to identify top k semantically relevant categorized review texts for each customer comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d1b16-2023-4729-88be-25f3cde6a490",
   "metadata": {},
   "source": [
    "- Test run Similarity search in vector store\n",
    "- Performing a simple similarity search with relevance score can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8ba65-2722-4b69-8895-f383e4b5f958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = comments['comment'].sample(1).values[0]\n",
    "print(colored(f\"******query*****:\\n{query}\",\"blue\"))\n",
    "\n",
    "results = vector_store.similarity_search_with_relevance_scores(query, k=4)\n",
    "print(colored(\"\\n\\n******results*****\",\"green\"))\n",
    "for res, score in results:\n",
    "    print(colored(f\"* [SIM={score:3f}] \\n{res.page_content}\\n{res.metadata}\",\"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee04b66-3e44-43ef-9188-1d7efb3ed4ec",
   "metadata": {},
   "source": [
    "## 5. Build a RAG chain to retrieve only top K relevant examples for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df19799-0bb8-4676-86ea-c112388bd1d5",
   "metadata": {},
   "source": [
    "#### 5.1 Customzie a LangChain Chat Model Class\n",
    "- As the time of this event, LangChain has not supported the latest Amazon Foundation Model yet, we will customize a LangChain Chat Model Class, so that the latest model can be integrated with the chain prompting in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36278dd7-b222-48ca-b0f9-d73b36a0b45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage,AIMessage,SystemMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser,XMLOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder,HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "class ChatModelOly(BaseChatModel):\n",
    "\n",
    "    model_name: str\n",
    "    br_runtime : Any = None\n",
    "    ak: str = None\n",
    "    sk: str = None\n",
    "    region:str = None\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "\n",
    "        if not self.br_runtime:\n",
    "            if self.ak and self.sk:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime',\n",
    "                                               region_name = self.region,\n",
    "                                              aws_access_key_id = self.ak,\n",
    "                                               aws_secret_access_key = self.sk\n",
    "            \n",
    "                                              )\n",
    "\n",
    "            else:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime')\n",
    "            \n",
    "        \n",
    "        new_messages = []\n",
    "        system_message = ''\n",
    "        for msg in messages:\n",
    "            if isinstance(msg,SystemMessage):\n",
    "                system_message = msg.content\n",
    "            elif isinstance(msg,HumanMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "            elif isinstance(msg,AIMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "\n",
    "        \n",
    "        temperature = kwargs.get('temperature',0.1)\n",
    "        maxTokens = kwargs.get('max_tokens',3000)\n",
    "\n",
    "        #Base inference parameters to use.\n",
    "        inference_config = {\"temperature\": temperature,\"maxTokens\":maxTokens}\n",
    "\n",
    "\n",
    "        # Send the message.\n",
    "        response = self.br_runtime.converse(\n",
    "            modelId=self.model_name,\n",
    "            messages=new_messages,\n",
    "            system=[{\"text\" : system_message}] if system_message else [],\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        output_message = response['output']['message']\n",
    "\n",
    "        message = AIMessage(\n",
    "            content=output_message['content'][0]['text'],\n",
    "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
    "            response_metadata={  # Use for response metadata\n",
    "                **response['usage']\n",
    "            },\n",
    "        )\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        if not self.br_runtime:\n",
    "            if self.ak and self.sk:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime',\n",
    "                                               region_name = self.region,\n",
    "                                              aws_access_key_id = self.ak,\n",
    "                                               aws_secret_access_key = self.sk\n",
    "            \n",
    "                                              )\n",
    "\n",
    "            else:\n",
    "                self.br_runtime = boto3.client(service_name = 'bedrock-runtime')\n",
    "            \n",
    "        \n",
    "        new_messages = []\n",
    "        system_message = ''\n",
    "        for msg in messages:\n",
    "            if isinstance(msg,SystemMessage):\n",
    "                system_message = msg.content\n",
    "            elif isinstance(msg,HumanMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "            elif isinstance(msg,AIMessage):\n",
    "                new_messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [ {\"text\": msg.content}]\n",
    "                    })\n",
    "\n",
    "        \n",
    "        temperature = kwargs.get('temperature',0.1)\n",
    "        maxTokens = kwargs.get('max_tokens',3000)\n",
    "\n",
    "        #Base inference parameters to use.\n",
    "        inference_config = {\"temperature\": temperature,\"maxTokens\":maxTokens}\n",
    "\n",
    "        # Send the message.\n",
    "        streaming_response = self.br_runtime.converse_stream(\n",
    "            modelId=self.model_name,\n",
    "            messages=new_messages,\n",
    "            system=[{\"text\" : system_message}] if system_message else [],\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        # Extract and print the streamed response text in real-time.\n",
    "        for event in streaming_response[\"stream\"]:\n",
    "            if \"contentBlockDelta\" in event:\n",
    "                text = event[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "                # print(text, end=\"\")\n",
    "                chunk = ChatGenerationChunk(message=AIMessageChunk(content=[{\"type\":\"text\",\"text\":text}]))\n",
    "\n",
    "                if run_manager:\n",
    "                    # This is optional in newer versions of LangChain\n",
    "                    # The on_llm_new_token will be called automatically\n",
    "                    run_manager.on_llm_new_token(text, chunk=chunk)\n",
    "\n",
    "                yield chunk\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                # Let's add some other information (e.g., response metadata)\n",
    "                chunk = ChatGenerationChunk(\n",
    "                    message=AIMessageChunk(content=\"\", response_metadata={**metadata})\n",
    "                )\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_new_token(text, chunk=chunk)\n",
    "                yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "\n",
    "llm = ChatModelOly(model_name=\"amazon.olympus-pro-v1:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabf9d6-4b8b-4f46-b75d-d07b41d5f114",
   "metadata": {},
   "source": [
    "- Define system prompt and user prompt template\n",
    "\n",
    "The system prompt template has been copied over from Lab 1, while the user prompt has been modified slightly. For this user prompt, we add a part where we provide examples to the LLM. This is a demonstration for Few-Shot Learning. The model will takes these exmaples as references when making classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366716c1-8d0f-433b-8d96-b4ca5c29282a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system = \"\"\"You are a professional  customer feedback analyst. Your daily task is to categorize user feedback.\n",
    "You will be given an input in the form of a JSON array. Each object in the array contains a comment ID and a 'c' field representing the user's comment content.\n",
    "Your role is to analyze these comments and categorize them appropriately.\n",
    "Please note:\n",
    "1. Only output valid XML format data.\n",
    "2. Do not include any explanations or additional text outside the XML structure.\n",
    "3. Ensure your categorization is accurate and consistent.\n",
    "4. If you encounter any ambiguous cases, use your best judgment based on the context provided.\n",
    "5. Maintain a professional and neutral tone in your categorizations.\n",
    "\"\"\"\n",
    "\n",
    "user = \"\"\"\n",
    "Please categorize user comments according to the following category tags library:\n",
    "<categories>\n",
    "{tags}\n",
    "</categories>\n",
    "\n",
    "Here are examples for your to categorize:\n",
    "<examples>\n",
    "{examples}\n",
    "<examples>\n",
    "\n",
    "Please follow these instructions for categorization:\n",
    "<instruction>\n",
    "1. Categorize each comment using the tags above. If no tags apply, output \"Invalid Data\".\n",
    "2. Summarize the comment content in no more than 50 words. Replace any double quotation marks with single quotation marks.\n",
    "</instruction>\n",
    "\n",
    "Below are the customer comments records to be categorized. The input is an array, where each element has an 'id' field representing the complaint ID and a 'c' field summarizing the complaint content.\n",
    "<comments>\n",
    "{input}\n",
    "</comments>\n",
    "\n",
    "For each record, summarize the comment, categorize according to the category explainations, and return the  ID, summary , reasons for tag matches, and category.\n",
    "\n",
    "Output format example:\n",
    "<output>\n",
    "  <item>\n",
    "    <id>xxx</id>\n",
    "    <summary>xxx</summary>\n",
    "    <reason>xxx</reason>\n",
    "    <category>xxx</category>\n",
    "  </item>\n",
    "</output>\n",
    "\n",
    "Skip the preamble and output only valid XML format data. Remember:\n",
    "- Avoid double quotation marks within quotation marks. Use single quotation marks instead.\n",
    "- Replace any double quotation marks in the content with single quotation marks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854dca9-967f-4431-91c5-71d95f1928f3",
   "metadata": {},
   "source": [
    "#### 5.2 Define a RAG pipeline with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef9b12",
   "metadata": {},
   "source": [
    "We are now set to create a RAG pipeline using Langchain. The workflow of this pipelines involves:\n",
    "- initially, it retrieves the top k most relevant customer review texts, which are labeled with categories, from the vector database. \n",
    "- This retrieved information is then integrated into the prompt. \n",
    "- Invoke LLM to generate outputs according to the provided instructions. \n",
    "- Finally, the output is displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917948d5-a272-45d3-bc17-7f66bee7c826",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Define langchain retriever, use `k = 5` to retrive top 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fd9d2-a681-45c9-a215-d626d672d05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7444e5-4e94-4510-8430-ed53f84fd8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted = \"\\n\".join([f\"<content>{doc.page_content}</content>\\n<cateogry>{doc.metadata['groundtruth']}</cateogry>\" for doc in docs])\n",
    "    # print(colored(f\"*****retrived examples******:\\n{formatted}\",\"yellow\"))\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f11e0b-97f4-4f8f-9675-adc413bafc0a",
   "metadata": {},
   "source": [
    "- test run for retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2eccb4-f060-468b-a167-e9875cce6bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_docs = (retriever|format_docs).invoke(\"my phone always freeze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc18f8-aa6c-40c3-b054-2209f71a80b6",
   "metadata": {},
   "source": [
    "- Define prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f67a9c-26dd-41b5-af32-ccc89a015ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    ('system',system),\n",
    "    ('user',user)],\n",
    "partial_variables={'tags':categories['mappings'].values}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eed683-f827-4e10-9eea-dab05e90e6ea",
   "metadata": {},
   "source": [
    "- Define a RAG prompt chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537e57f-0c13-4506-91f9-a86e419e0aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"examples\":retriever|format_docs,\"input\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | XMLOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34286b-88ae-4bc6-ab2c-e462062f41f6",
   "metadata": {},
   "source": [
    "- Convert comments into a data array for iterative processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e0218-4c56-4e15-ab9b-39896724d6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data = [str({\"id\":'s_'+str(i),\"comment\":x[0]}) for i,x in enumerate(comments.values)]\n",
    "print(sample_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea586e60-a676-468b-86c4-01483e2042a3",
   "metadata": {},
   "source": [
    "### We will iterate through each comment for categorization.\n",
    "* This step will take around 4~5 mins to compelete\n",
    "* <span style=\"color: red;\">Hint!</span> Due to the uncertainly of LLM generation, the JSON output might fail occasionally, if you happen to experience error such as `KeyError: 'output'`, please re-run the cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b73eb3-6ddb-4507-962b-8bac8f1f5e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import math,json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "resps = []\n",
    "\n",
    "\n",
    "def retry_call(chain,args: Dict[str,Any],times:int=3):\n",
    "    \"\"\"\n",
    "      Retry mechanism to ensure the success rate of final structure output \n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = chain.invoke(args)\n",
    "        if 'output' not in content:\n",
    "            raise (JSONDecodeError('KeyError: output'))\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        if times:\n",
    "            print(f'Exception, return [{times}]')\n",
    "            return retry_call(chain,args,times=times-1)\n",
    "        else:\n",
    "            raise(JSONDecodeError(e))\n",
    "\n",
    "for i in range(len(sample_data)):\n",
    "    data = sample_data[i]\n",
    "    # print(colored(f\"*****input[{i}]*****:\\n{data}\",\"blue\"))\n",
    "\n",
    "    # resp = chain.invoke(data)\n",
    "    resp = retry_call(chain,data)\n",
    "    print(colored(f\"*****response*****\\n{resp}\",\"green\"))\n",
    "    # resps += json.loads(resp)\n",
    "    for item in resp['output']:\n",
    "        row={}\n",
    "        for it in item['item']:\n",
    "            row[list(it.keys())[0]]=list(it.values())[0]\n",
    "        resps.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d68757",
   "metadata": {},
   "source": [
    "### Check if all the data has been processed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(resps) == len(sample_data), \"Due to the uncertainly of LLM generation, the JSON output might fail occasionally, if you happen to experience error, please re-run above cell again\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63710dd1-82b0-49aa-9051-0c2d7cb2a23d",
   "metadata": {},
   "source": [
    "- covert the data array to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec70ebe-85a3-4670-962b-f728ef537b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(resps).rename(columns={\"category\":\"predict_label\"}).drop_duplicates(['id']).reset_index(drop='index')\n",
    "# convert the label value to lowercase\n",
    "prediction_df['predict_label'] = prediction_df['predict_label'].apply(lambda x: x.strip().lower().replace(\"'\",\"\"))\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325043e-5c6b-43a9-bd37-16d2abfb45da",
   "metadata": {},
   "source": [
    "### Merge the prediction result to ground truth dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df39a0-09d9-48b9-8423-c19386a011c8",
   "metadata": {},
   "source": [
    "- copy comments to ground_truth dataframe\n",
    "- merge the date prediction to the groudtruth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fff95d-db36-4254-84b6-6e4f83df3d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth = comments.copy()\n",
    "# convert the label value to lowercase\n",
    "ground_truth['groundtruth'] = ground_truth['groundtruth'].apply(lambda x: x.strip().lower())\n",
    "merge_df=pd.concat([ground_truth,prediction_df],axis=1)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ece865-31d2-4d08-a7db-5108ccc1f8be",
   "metadata": {},
   "source": [
    "## 6.Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c68f3-f836-46cf-8a68-92d5d9be96b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = (merge_df['groundtruth'] == merge_df['predict_label']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9aace7-401e-4da3-a9da-50b8b10bc578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(colored(f\"****accuracy:****\\n{accuracy}\",\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b84f30-53a4-42fe-947a-fd095d004b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### save the result \n",
    "merge_df.to_csv('result_lab_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f581e37-2c77-4223-869d-001a8e05f393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
